# Approach
I used a hybrid approach that combines lightweight rule-based processing with an LLM. From reading multiple ADs, I realized the documents vary in structure. A fully rule-based approach breaks easily, but using an LLM end-to-end wasn’t right either because the outputs are inconsistent and LLM often produces hallucinated results. So instead of choosing one over the other, I tried to combine them. Rules handle structure-related problems like cleaning the text and grouping it at the paragraph level, while the LLM is only used when interpretation is unavoidable. Each paragraph is tagged into a small set of coarse categories such as AD ID, Applicability, and Reason. Based on the tag, the LLM receives a specific instruction, so it knows exactly what kind of information it is supposed to extract rather than answering freely.

# Challenges
My main challenge was that AD documents are structured differently from one document to another and often mix different types of information within the same section or paragraph. Keywords are often ambiguous. For example, the word "aircraft" appears in both Applicability sections ("applies to Boeing 737 aircraft") and Reason sections ("aircraft may experience structural failure"). Simple exact matching would mis-tag these. I handled this by assigning scores instead of relying on exact matches. Strong indicators such as AD number patterns or section-like phrases were given higher scores, while more generic words contributed smaller scores. Tags were assigned based on the total score rather than a single match. Another challenge was stabilizing the LLM’s output. I addressed this by only giving the LLM already-tagged paragraphs and using tag-specific instructions to narrow its focus. Some edge cases appeared due to PDF extraction noise or non-standard formatting, such as broken AD numbers or unusually long paragraphs. In these cases, tagging confidence drops, so I either rely more on strong patterns (e.g., regex-based AD number detection) or defer interpretation to the LLM with stricter, tag-specific instructions instead of forcing a rule-based decision.

# Limitations
My approach has a few limitations. It depends on predefined keywords and patterns, so if an AD uses unexpected terminology or structure, the tagging step can become less reliable. Another limitation is that very long ADs (100+ pages) could create oversized inputs when I concatenate all same-tag paragraphs together, potentially exceeding context limits. With more time, I would explore using RAG to retrieve similar AD paragraphs as examples for the LLM, which could help handle terminology variations that my keyword lists don’t cover. However, this would require additional hardware (embedding model and vector database) and careful tuning to ensure retrieved examples are relevant to the extraction task.

# Trade-offs
I used a hybrid approach combining an LLM with rule-based tagging for structured parts. The LLM can extract accurate information when given good prompts. I didn’t use the LLM end-to-end because its outputs would be inconsistent, and I didn’t rely solely on rule-based methods because they would break if there are format variations. I also didn’t use VLMs due to hardware limitations and my limited familiarity with them, which would have required more time to implement. I chose llama3.1:8b instead of larger models like Claude or GPT for cost reasons. While more accurate models exist, LLaMA llama3.1:8b provides sufficient accuracy for this task when given focused, tagged context.